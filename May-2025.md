# All Learnings of May 2025

<details>
   <summary><strong>AI for Everyone Course - by Andrew Ng on Coursera</strong></summary>

## 20/5/25
- ANI, Gen AI and AGI
- Supervised Learning
- How do LLMs work?
- Importance of Cleaning up data before feeding it to system
- ML v/s DS
- What is Deep Learning / Neural Networks?

## 21/5/25
- Starting an AI project: Workflows for ML and DS projects
- Brainstorming Framework: How can businesses use AI to be more efficient
- Build v/s Buy
- Working with an AI team
- Various Libraris/Tools: PyTorch, TensorFlow, HuggingFace, Paddle Paddle, Scikit-Learn, R, Research Pubilication on arxiv, Repos: Github
- Building AI in your company: Case Studies for Smart Speackers and Self-Driving Cars
- Different Roles for AI: Software Engineer, ML Engineer, ML Researcher, Data Scientist, Data Engineer, AI Product Manager
- AlexNet and its papers

## 22/5/25
- Execute Pilot Projcts: more important for initial projects to succeed rather than be most valuable
- Show traction within 6-12 months
- Who is CAIO: Chief AI Officer looks upon the in-house AI team which develops solutions for other business units
- Providing AI training for executives, senior business leaders, leaders of divitions and trainees too is very important
- Better Product -> More Users -> More Data -> Better Product and the cycle continues
- Don't be too optimistic or pessimistic about AI. It can't solve everything? At the same time, it can create great impact for very specific applications
- Get some friends to learn about AI
- Start brainstorming projects with them: No project is too small to start
- Areas of Impact: Computer Vision, NLP, Speech, Generative AI, Robotics, General ML, Unsupervised Learning, Transfer Learning, Reinforcement Learning, GAN, Knowledge graphs etc.
- Limitations of AI: Biases, performance issues, adversarial attacks, deepfakes etc.
</details>

<details>
  <summary><strong>How I use LLMs by Andrew Karpathy on YT</strong></summary>

  ## 23/5/25
  - User input -> LLM like ChatGPT (Generative Pre-trained Transformer) -> Output
  - Basically LLMs predict the next works in a sentence as we type.
  - How user input is divided into tokens? Use [Tiktokenizer](https://tiktokenizer.vercel.app/) to actually what's happening under the hood
  - LLMs are usually out of date by a few months.
  - For every 1TB data trained on LLM, there will be trillions of parameters that can be fine tuned.
  - What is pre-training and what is post-training?

  ## Extra
  - I learnt about OpenAI's API keys. I will use them sooner of later.
</details>
